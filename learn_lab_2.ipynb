{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "dataset_name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                 transform=T.ToSparseTensor())\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "\n",
    "data.adj_t=data.adj_t.to_symmetric()\n",
    "split_idx=dataset.get_idx_split()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_idx=split_idx['train'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "![picture](cs224w-colab2-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models <-Construct the network as showing in the figure\n",
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim,output_dim,num_layers,dropout):\n",
    "    super(GCN, self).__init__()\n",
    "    self.convs=None\n",
    "    self.bns=None\n",
    "    self.dropout=dropout\n",
    "    self.num_layers=num_layers\n",
    "    self.convs=torch.nn.ModuleList()\n",
    "    self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "    for i in range(num_layers-2):\n",
    "      self.convs.append(GCNConv(hidden_dim,hidden_dim))\n",
    "    self.convs.append(GCNConv(hidden_dim,output_dim))\n",
    "    self.bns=torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers-1)])\n",
    "    self.softmax=torch.nn.LogSoftmax()\n",
    "  \n",
    "  def reset_parameters(self):\n",
    "    for conv in self.convs:\n",
    "      conv.reset_parameters()\n",
    "    for bn in self.bns:\n",
    "      bn.reset_parameters()\n",
    "  def forward(self,x, adj_t):\n",
    "    for i in range(self.num_layers-1):\n",
    "      x=self.convs[i](x, adj_t)\n",
    "      x=self.bns[i](x)\n",
    "      x=F.relu(x)\n",
    "      x=F.dropout(x,self.dropout,self.training)\n",
    "    x=self.convs[-1](x,adj_t)\n",
    "    x=self.softmax(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169343, 128])\n",
      "torch.Size([90941, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([169343])\n",
      "torch.Size([90941])\n"
     ]
    }
   ],
   "source": [
    "print(data.x.shape)\n",
    "#取多行\n",
    "print(data.x[train_idx].shape)\n",
    "#取一行 降维\n",
    "print(data.x[0].shape)\n",
    "#取一列 降维\n",
    "print(data.x[:,0].shape)\n",
    "print(data.x[train_idx,0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement this function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slicing the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    optimizer.zero_grad()\n",
    "    #x=data.x[train_idx]\n",
    "    y=data.y[train_idx,0]\n",
    "    #why  model(data.x[train_idx]) is wrong ?\n",
    "    l=loss_fn(model(data.x,data.adj_t)[train_idx],y)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    return l.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    # TODO: Implement this function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "    #Sets the module in evaluation mode.\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note:\n",
    "    ## 1. No index slicing here\n",
    "    out=model(data.x,data.adj_t)\n",
    "    #########################################\n",
    "    y_pred=torch.argmax(out,dim=-1,keepdim=True)\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "    return train_acc, valid_acc, test_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'num_layers': 3,\n",
       " 'hidden_dim': 256,\n",
       " 'dropout': 0.5,\n",
       " 'lr': 0.01,\n",
       " 'epochs': 100}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparameters\n",
    "args = {\n",
    "    'device': device,\n",
    "    'num_layers': 3,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 100,\n",
    "}\n",
    "#instanize\n",
    "model = GCN(data.num_features, args['hidden_dim'],\n",
    "            dataset.num_classes, args['num_layers'],\n",
    "            args['dropout']).to(device)\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug module is\n",
    "#model(data.x,data.adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-80373950429f>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.260563850402832"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),args['lr'])\n",
    "loss_fn=F.nll_loss\n",
    "#train(model,data,train_idx, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-80373950429f>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:00, train_acc:0.14, valid_acc:0.24 test_acc:0.22\n",
      "epoch:01, train_acc:0.26, valid_acc:0.23 test_acc:0.28\n",
      "epoch:02, train_acc:0.24, valid_acc:0.24 test_acc:0.24\n",
      "epoch:03, train_acc:0.22, valid_acc:0.16 test_acc:0.15\n",
      "epoch:04, train_acc:0.26, valid_acc:0.16 test_acc:0.15\n",
      "epoch:05, train_acc:0.30, valid_acc:0.19 test_acc:0.18\n",
      "epoch:06, train_acc:0.36, valid_acc:0.27 test_acc:0.26\n",
      "epoch:07, train_acc:0.42, valid_acc:0.34 test_acc:0.34\n",
      "epoch:08, train_acc:0.45, valid_acc:0.38 test_acc:0.37\n",
      "epoch:09, train_acc:0.46, valid_acc:0.41 test_acc:0.41\n",
      "epoch:10, train_acc:0.46, valid_acc:0.45 test_acc:0.47\n",
      "epoch:11, train_acc:0.45, valid_acc:0.44 test_acc:0.46\n",
      "epoch:12, train_acc:0.45, valid_acc:0.43 test_acc:0.44\n",
      "epoch:13, train_acc:0.46, valid_acc:0.43 test_acc:0.43\n",
      "epoch:14, train_acc:0.48, valid_acc:0.46 test_acc:0.45\n",
      "epoch:15, train_acc:0.52, valid_acc:0.50 test_acc:0.49\n",
      "epoch:16, train_acc:0.55, valid_acc:0.54 test_acc:0.54\n",
      "epoch:17, train_acc:0.57, valid_acc:0.56 test_acc:0.58\n",
      "epoch:18, train_acc:0.58, valid_acc:0.58 test_acc:0.60\n",
      "epoch:19, train_acc:0.59, valid_acc:0.59 test_acc:0.61\n",
      "epoch:20, train_acc:0.60, valid_acc:0.60 test_acc:0.62\n",
      "epoch:21, train_acc:0.61, valid_acc:0.62 test_acc:0.63\n",
      "epoch:22, train_acc:0.62, valid_acc:0.63 test_acc:0.64\n",
      "epoch:23, train_acc:0.63, valid_acc:0.64 test_acc:0.65\n",
      "epoch:24, train_acc:0.64, valid_acc:0.65 test_acc:0.65\n",
      "epoch:25, train_acc:0.65, valid_acc:0.66 test_acc:0.65\n",
      "epoch:26, train_acc:0.66, valid_acc:0.66 test_acc:0.65\n",
      "epoch:27, train_acc:0.67, valid_acc:0.67 test_acc:0.66\n",
      "epoch:28, train_acc:0.67, valid_acc:0.67 test_acc:0.67\n",
      "epoch:29, train_acc:0.68, valid_acc:0.68 test_acc:0.67\n",
      "epoch:30, train_acc:0.68, valid_acc:0.68 test_acc:0.68\n",
      "epoch:31, train_acc:0.68, valid_acc:0.68 test_acc:0.68\n",
      "epoch:32, train_acc:0.68, valid_acc:0.68 test_acc:0.68\n",
      "epoch:33, train_acc:0.68, valid_acc:0.68 test_acc:0.68\n",
      "epoch:34, train_acc:0.69, valid_acc:0.69 test_acc:0.68\n",
      "epoch:35, train_acc:0.69, valid_acc:0.69 test_acc:0.69\n",
      "epoch:36, train_acc:0.69, valid_acc:0.69 test_acc:0.69\n",
      "epoch:37, train_acc:0.70, valid_acc:0.70 test_acc:0.69\n",
      "epoch:38, train_acc:0.70, valid_acc:0.70 test_acc:0.69\n",
      "epoch:39, train_acc:0.70, valid_acc:0.70 test_acc:0.69\n",
      "epoch:40, train_acc:0.70, valid_acc:0.70 test_acc:0.69\n",
      "epoch:41, train_acc:0.70, valid_acc:0.70 test_acc:0.69\n",
      "epoch:42, train_acc:0.71, valid_acc:0.70 test_acc:0.69\n",
      "epoch:43, train_acc:0.71, valid_acc:0.70 test_acc:0.68\n",
      "epoch:44, train_acc:0.71, valid_acc:0.70 test_acc:0.69\n",
      "epoch:45, train_acc:0.71, valid_acc:0.70 test_acc:0.69\n",
      "epoch:46, train_acc:0.71, valid_acc:0.71 test_acc:0.70\n",
      "epoch:47, train_acc:0.71, valid_acc:0.70 test_acc:0.69\n",
      "epoch:48, train_acc:0.71, valid_acc:0.70 test_acc:0.68\n",
      "epoch:49, train_acc:0.71, valid_acc:0.70 test_acc:0.68\n",
      "epoch:50, train_acc:0.71, valid_acc:0.70 test_acc:0.68\n",
      "epoch:51, train_acc:0.71, valid_acc:0.70 test_acc:0.69\n",
      "epoch:52, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:53, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:54, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:55, train_acc:0.72, valid_acc:0.70 test_acc:0.68\n",
      "epoch:56, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:57, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:58, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:59, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:60, train_acc:0.72, valid_acc:0.70 test_acc:0.68\n",
      "epoch:61, train_acc:0.72, valid_acc:0.70 test_acc:0.68\n",
      "epoch:62, train_acc:0.72, valid_acc:0.71 test_acc:0.70\n",
      "epoch:63, train_acc:0.72, valid_acc:0.71 test_acc:0.70\n",
      "epoch:64, train_acc:0.72, valid_acc:0.71 test_acc:0.70\n",
      "epoch:65, train_acc:0.72, valid_acc:0.70 test_acc:0.68\n",
      "epoch:66, train_acc:0.72, valid_acc:0.70 test_acc:0.69\n",
      "epoch:67, train_acc:0.72, valid_acc:0.71 test_acc:0.69\n",
      "epoch:68, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:69, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:70, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:71, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:72, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:73, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:74, train_acc:0.73, valid_acc:0.70 test_acc:0.68\n",
      "epoch:75, train_acc:0.73, valid_acc:0.70 test_acc:0.68\n",
      "epoch:76, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:77, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:78, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:79, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:80, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:81, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:82, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:83, train_acc:0.73, valid_acc:0.71 test_acc:0.69\n",
      "epoch:84, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:85, train_acc:0.73, valid_acc:0.72 test_acc:0.70\n",
      "epoch:86, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:87, train_acc:0.73, valid_acc:0.71 test_acc:0.70\n",
      "epoch:88, train_acc:0.73, valid_acc:0.72 test_acc:0.70\n",
      "epoch:89, train_acc:0.74, valid_acc:0.72 test_acc:0.71\n",
      "epoch:90, train_acc:0.74, valid_acc:0.71 test_acc:0.70\n",
      "epoch:91, train_acc:0.74, valid_acc:0.71 test_acc:0.70\n",
      "epoch:92, train_acc:0.74, valid_acc:0.72 test_acc:0.70\n",
      "epoch:93, train_acc:0.74, valid_acc:0.72 test_acc:0.71\n",
      "epoch:94, train_acc:0.74, valid_acc:0.72 test_acc:0.71\n",
      "epoch:95, train_acc:0.74, valid_acc:0.71 test_acc:0.69\n",
      "epoch:96, train_acc:0.74, valid_acc:0.71 test_acc:0.69\n",
      "epoch:97, train_acc:0.74, valid_acc:0.72 test_acc:0.70\n",
      "epoch:98, train_acc:0.73, valid_acc:0.71 test_acc:0.71\n",
      "epoch:99, train_acc:0.74, valid_acc:0.72 test_acc:0.71\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "import copy\n",
    "# reset the parameters to initial random value\n",
    "model.reset_parameters()\n",
    "optimizer=torch.optim.Adam(model.parameters(),args['lr'])\n",
    "loss_fn=F.nll_loss\n",
    "\n",
    "best_acc=0\n",
    "best_model=None\n",
    "for epoch in range(args['epochs']):\n",
    "  loss=train(model,data,train_idx, optimizer, loss_fn)\n",
    "  train_acc, valid_acc, test_acc=test(model,data,split_idx,evaluator)\n",
    "  if valid_acc>best_acc:\n",
    "    best_acc=valid_acc\n",
    "    best_model=copy.deepcopy(model)\n",
    "  print(f'epoch:{epoch:02d},',f'train_acc:{train_acc:.2f},',f'valid_acc:{valid_acc:.2f}',f'test_acc:{test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph level\n",
    "dataprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a65a5d72b0551654d0f0fd1b19df23ecb14742447f281acd582799b77c8baba5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
